{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from toolz.curried import pipe, curry, compose\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample.py\n",
    "from torchvision import transforms \n",
    "from build_vocab import Vocabulary\n",
    "#train.py\n",
    "from data_loader import get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    if args.cuda:\n",
    "        U = U.cuda(device)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False, latent_dim=None, categorical_dim=None):\n",
    "    \"\"\"\n",
    "    ST-gumple-softmax\n",
    "    input: [*, n_class]\n",
    "    return: flatten --> [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    if not hard:\n",
    "        return y\n",
    "        return y.view(-1, latent_dim * categorical_dim)\n",
    "\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "    y_hard = (y_hard - y).detach() + y\n",
    "    return y_hard\n",
    "    return y_hard.view(-1, latent_dim * categorical_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        hiddens = pad_packed_sequence(hiddens, batch_first=True)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    " - 9956 words in our vocabulary\n",
    " - created a vocab class with word_to_idx and idx_to_word attributes (dicts) using pycocotools\n",
    " - a minimum word count threshold of 4 was chosen as default value to identify and remove rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try some model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define parameters and filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    model_path = 'nb_models'\n",
    "    crop_size = 224\n",
    "    vocab_path = 'data/vocab.pkl'\n",
    "    image_dir = 'data/resized2014'\n",
    "    caption_path = 'data/annotations/captions_train2014.json'\n",
    "    log_step = 10\n",
    "    save_step = 1000\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    num_layers = 1\n",
    "    num_epochs = 1\n",
    "    batch_size = 128\n",
    "    num_workers = 4\n",
    "    learning_rate = 0.001\n",
    "    cuda = True\n",
    "args = Args()\n",
    "\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "    \n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing, normalization for the pretrained resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ \n",
    "    transforms.RandomCrop(args.crop_size),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vocabulary wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 9956\n"
     ]
    }
   ],
   "source": [
    "with open(args.vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "print(\"Size of Vocabulary: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.14s)\n",
      "creating index...\n",
      "index created!\n",
      "CPU times: user 849 ms, sys: 132 ms, total: 981 ms\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_loader = get_loader(args.image_dir, args.caption_path, vocab, \n",
    "                         transform, args.batch_size,\n",
    "                         shuffle=True, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 s, sys: 467 ms, total: 1.75 s\n",
      "Wall time: 1.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encoder = EncoderCNN(args.embed_size).to(device)\n",
    "decoder = DecoderRNN(args.embed_size, args.hidden_size, len(vocab), args.num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 147 ms, sys: 78 ms, total: 225 ms\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encoder_path = \"models/encoder-2-1000.ckpt\"\n",
    "decoder_path = \"models/decoder-2-1000.ckpt\"\n",
    "encoder.load_state_dict(torch.load(encoder_path, map_location=torch.device(\"cpu\")))\n",
    "decoder.load_state_dict(torch.load(decoder_path, map_location=torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "CPU times: user 21 ms, sys: 158 ms, total: 179 ms\n",
      "Wall time: 4.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "    print(i)\n",
    "    if i > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.46 p.m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input -> minibatch of images:  torch.Size([128, 3, 224, 224])\n",
      "input -> minibatch of captions:  torch.Size([128, 31])\n",
      "input -> minibatch of lengths:  128\n"
     ]
    }
   ],
   "source": [
    "print(\"input -> minibatch of images: \", images.shape)\n",
    "print(\"input -> minibatch of captions: \", captions.shape)\n",
    "print(\"input -> minibatch of lengths: \", len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target shape -> : torch.Size([1721])\n"
     ]
    }
   ],
   "source": [
    "images = images.to(device)\n",
    "captions = captions.to(device)\n",
    "targets = pack_padded_sequence(captions, lengths, batch_first=True)\n",
    "print(\"target shape -> :\", targets[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate deep features for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image features: ->  torch.Size([128, 256])\n"
     ]
    }
   ],
   "source": [
    "ifeatures = encoder(images)\n",
    "print(\"Shape of image features: -> \", ifeatures.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings (latent space representation) for the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of caption embeddings: -> torch.Size([128, 31, 9956])\n"
     ]
    }
   ],
   "source": [
    "wfeatures = decoder(ifeatures, captions, lengths)\n",
    "print(\"Shape of caption embeddings: ->\", wfeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 1.0\n",
    "hard = False\n",
    "latent_dim = wfeatures.shape[1]\n",
    "categorical_dim = wfeatures.shape[2]\n",
    "wfeatures_gsm = gumbel_softmax(wfeatures, temp, hard, latent_dim, categorical_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packed = pack_padded_sequence(wfeatures, lengths, batch_first=True)\n",
    "# print(packed[0].shape)\n",
    "# print(criterion(packed[0], targets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1721, 9956])\n"
     ]
    }
   ],
   "source": [
    "packed = pack_padded_sequence(wfeatures_gsm, lengths, batch_first=True)\n",
    "print(packed[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(embed_size, hidden_size)\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        wembeddings = self.embed(captions)\n",
    "        wembeddings = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        wembeddings, _ = self.lstm(packed)\n",
    "        hiddens = pad_packed_sequence(hiddens, batch_first=True)\n",
    "        \n",
    "        wembeddings = pipe(captions, \n",
    "                           lambda x: self.embed(x), \n",
    "                           lambda x: pack_padded_sequence(embeddings, lengths, batch_first=True))\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    A CaptioningRNN produces captions from image features using a recurrent\n",
    "    neural network.\n",
    "\n",
    "    The RNN receives input vectors of size D, has a vocab size of V, works on\n",
    "    sequences of length T, has an RNN hidden dimension of H, uses word vectors\n",
    "    of dimension W, and operates on minibatches of size N.\n",
    "\n",
    "    Note that we don't use any regularization for the CaptioningRNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 word_to_idx, \n",
    "                 input_Dim=1, \n",
    "                 wordvec_Dim=128,\n",
    "                 hidden_Dim=128,\n",
    "                 num_layers=1,\n",
    "                 N=128, \n",
    "                 O=128, \n",
    "                 image_input=512, \n",
    "                 set_size=10, \n",
    "                 use_cuda = False,\n",
    "                 device = torch.device(\"cpu\")):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            self.use_cuda = True\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            \n",
    "        self.sentence_embedding= CaptioningModel(word_to_idx, \n",
    "                                                 input_dim=input_Dim, \n",
    "                                                 wordvec_dim=wordvec_Dim,\n",
    "                                                 num_layers = num_layers,\n",
    "                                                 hidden_dim=hidden_Dim, \n",
    "                                                 use_cuda = self.use_cuda, \n",
    "                                                 device = self.device)         \n",
    "        \n",
    "        vocab_size=len(word_to_idx)\n",
    "        self.distance_layer_sentences = Distance_Sentences(vocab_size, N, O)\n",
    "        self.distance_layer_images = Distance_Image(vocab_size, N, O, image_input)\n",
    "        \n",
    "        \n",
    "        self.set_size = set_size\n",
    "        self.projection = nn.Linear((self.set_size+1)*O, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, captions, features):\n",
    "        \n",
    "        nsamples, _ = captions.shape\n",
    "        \n",
    "        ft = torch.zeros(nsamples,1).to(self.device)\n",
    "        print(\"input to embedding\", ft.shape)\n",
    "        \n",
    "        S = self.sentence_embedding.forward(ft, captions)\n",
    "        print(\"Sentence Embedding Shape\", S.shape)\n",
    "        \n",
    "        S = S[:,-1,:]\n",
    "        print(\"Sentence Embedding Modified Shape\", S.shape)\n",
    "        \n",
    "        S = S.view(S.shape[0] // self.set_size, self.set_size, -1)\n",
    "        print(S.shape)\n",
    "        \n",
    "        o_sentence = self.distance_layer_sentences.forward(S)\n",
    "        print(\"sentence distance\", o_sentence.shape)\n",
    "\n",
    "        o_image = self.distance_layer_images.forward(S,features)\n",
    "        print(\"image distance\", o_image.shape)\n",
    "        \n",
    "        o = torch.cat((o_image, o_sentence), 1).to(self.device)        \n",
    "        print(\"distances concatenated\", o.shape)\n",
    "        \n",
    "        D = nn.functional.log_softmax(self.projection(o),dim=1)\n",
    "        print(\"logmax performed\", D.shape)\n",
    "        \n",
    "        return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(data_loader)\n",
    "for epoch in range(args.num_epochs):\n",
    "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "        # Set mini-batch dataset\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print log info\n",
    "        if i % args.log_step == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                  .format(epoch, args.num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
    "\n",
    "#         # Save the model checkpoints\n",
    "#         if (i+1) % args.save_step == 0:\n",
    "#             torch.save(decoder.state_dict(), os.path.join(\n",
    "#                 args.model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "#             torch.save(encoder.state_dict(), os.path.join(\n",
    "#                 args.model_path, 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
