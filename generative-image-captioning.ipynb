{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample.py\n",
    "from torchvision import transforms \n",
    "from build_vocab import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py\n",
    "from data_loader import get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        hiddens = pad_packed_sequence(hiddens, batch_first=True)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    " - 9956 words in our vocabulary\n",
    " - created a vocab class with word_to_idx and idx_to_word attributes (dicts) using pycocotools\n",
    " - a minimum word count threshold of 4 was chosen as default value to identify and remove rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try some model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define parameters and filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    model_path = 'nb_models'\n",
    "    crop_size = 224\n",
    "    vocab_path = 'data/vocab.pkl'\n",
    "    image_dir = 'data/resized2014'\n",
    "    caption_path = 'data/annotations/captions_train2014.json'\n",
    "    log_step = 10\n",
    "    save_step = 1000\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "    num_layers = 1\n",
    "    num_epochs = 1\n",
    "    batch_size = 128\n",
    "    num_workers = 4\n",
    "    learning_rate = 0.001\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing, normalization for the pretrained resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ \n",
    "    transforms.RandomCrop(args.crop_size),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vocabulary wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 9956\n"
     ]
    }
   ],
   "source": [
    "with open(args.vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "print(\"Size of Vocabulary: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.86s)\n",
      "creating index...\n",
      "index created!\n",
      "CPU times: user 708 ms, sys: 168 ms, total: 876 ms\n",
      "Wall time: 2.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_loader = get_loader(args.image_dir, args.caption_path, vocab, \n",
    "                         transform, args.batch_size,\n",
    "                         shuffle=True, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(args.embed_size).to(device)\n",
    "decoder = DecoderRNN(args.embed_size, args.hidden_size, len(vocab), args.num_layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "CPU times: user 28 ms, sys: 142 ms, total: 170 ms\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "    print(i)\n",
    "    if i > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.46 p.m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input -> minibatch of images:  torch.Size([128, 3, 224, 224])\n",
      "input -> minibatch of captions:  torch.Size([128, 27])\n",
      "input -> minibatch of lengths:  128\n"
     ]
    }
   ],
   "source": [
    "print(\"input -> minibatch of images: \", images.shape)\n",
    "print(\"input -> minibatch of captions: \", captions.shape)\n",
    "print(\"input -> minibatch of lengths: \", len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target -> : torch.Size([1693])\n"
     ]
    }
   ],
   "source": [
    "images = images.to(device)\n",
    "captions = captions.to(device)\n",
    "targets = pack_padded_sequence(captions, lengths, batch_first=True)\n",
    "print(\"target -> :\", targets[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward, backward and optimize\n",
    "features = encoder(images)\n",
    "outputs = decoder(features, captions, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 256])\n",
      "torch.Size([128, 27, 9956])\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1693, 9956])\n",
      "tensor(9.2124, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "packed = pack_padded_sequence(outputs, lengths, batch_first=True)\n",
    "print(packed[0].shape)\n",
    "print(criterion(packed[0], targets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fede.distance_senteces import Distance_Sentences\n",
    "from fede.distance_image import Distance_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    A CaptioningRNN produces captions from image features using a recurrent\n",
    "    neural network.\n",
    "\n",
    "    The RNN receives input vectors of size D, has a vocab size of V, works on\n",
    "    sequences of length T, has an RNN hidden dimension of H, uses word vectors\n",
    "    of dimension W, and operates on minibatches of size N.\n",
    "\n",
    "    Note that we don't use any regularization for the CaptioningRNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 word_to_idx, \n",
    "                 input_Dim=1, \n",
    "                 wordvec_Dim=128,\n",
    "                 hidden_Dim=128,\n",
    "                 num_layers=1,\n",
    "                 N=128, \n",
    "                 O=128, \n",
    "                 image_input=512, \n",
    "                 set_size=10, \n",
    "                 use_cuda = False,\n",
    "                 device = torch.device(\"cpu\")):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            self.use_cuda = True\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            \n",
    "        self.sentence_embedding= CaptioningModel(word_to_idx, \n",
    "                                                 input_dim=input_Dim, \n",
    "                                                 wordvec_dim=wordvec_Dim,\n",
    "                                                 num_layers = num_layers,\n",
    "                                                 hidden_dim=hidden_Dim, \n",
    "                                                 use_cuda = self.use_cuda, \n",
    "                                                 device = self.device)         \n",
    "        \n",
    "        vocab_size=len(word_to_idx)\n",
    "        self.distance_layer_sentences = Distance_Sentences(vocab_size, N, O)\n",
    "        self.distance_layer_images = Distance_Image(vocab_size, N, O, image_input)\n",
    "        \n",
    "        \n",
    "        self.set_size = set_size\n",
    "        self.projection = nn.Linear((self.set_size+1)*O, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, captions, features):\n",
    "        \n",
    "        nsamples, _ = captions.shape\n",
    "        \n",
    "        ft = torch.zeros(nsamples,1).to(self.device)\n",
    "        print(\"input to embedding\", ft.shape)\n",
    "        S = self.sentence_embedding.forward(ft, captions)\n",
    "        print(\"Sentence Embedding Shape\", S.shape)\n",
    "        S = S[:,-1,:]\n",
    "        print(\"Sentence Embedding Modified Shape\", S.shape)\n",
    "        \n",
    "        S = S.view(S.shape[0] // self.set_size, self.set_size, -1)\n",
    "        print(S.shape)\n",
    "        \n",
    "        o_sentence = self.distance_layer_sentences.forward(S)\n",
    "        print(\"sentence distance\", o_sentence.shape)\n",
    "\n",
    "        o_image = self.distance_layer_images.forward(S,features)\n",
    "        print(\"image distance\", o_image.shape)\n",
    "        \n",
    "        o = torch.cat((o_image, o_sentence), 1).to(self.device)        \n",
    "        print(\"distances concatenated\", o.shape)\n",
    "        \n",
    "        D = nn.functional.log_softmax(self.projection(o),dim=1)\n",
    "        print(\"logmax performed\", D.shape)\n",
    "        \n",
    "        return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(data_loader)\n",
    "for epoch in range(args.num_epochs):\n",
    "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "        # Set mini-batch dataset\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print log info\n",
    "        if i % args.log_step == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                  .format(epoch, args.num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
    "\n",
    "        # Save the model checkpoints\n",
    "        if (i+1) % args.save_step == 0:\n",
    "            torch.save(decoder.state_dict(), os.path.join(\n",
    "                args.model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "            torch.save(encoder.state_dict(), os.path.join(\n",
    "                args.model_path, 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
