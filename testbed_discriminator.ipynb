{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import range\n",
    "from builtins import object\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "from fede.distance_senteces import Distance_Sentences\n",
    "from fede.distance_image import Distance_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptioningRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A CaptioningRNN produces captions from image features using a recurrent\n",
    "    neural network.\n",
    "\n",
    "    The RNN receives input vectors of size D, has a vocab size of V, works on\n",
    "    sequences of length T, has an RNN hidden dimension of H, uses word vectors\n",
    "    of dimension W, and operates on minibatches of size N.\n",
    "\n",
    "    Note that we don't use any regularization for the CaptioningRNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 word_to_idx, \n",
    "                 input_dim=512, \n",
    "                 wordvec_dim=128,\n",
    "                 hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Construct a new CaptioningRNN instance.\n",
    "\n",
    "        Inputs:\n",
    "        - word_to_idx: A dictionary giving the vocabulary. It contains V entries,\n",
    "          and maps each string to a unique integer in the range [0, V).\n",
    "        - input_dim: Dimension D of input image feature vectors.\n",
    "        - wordvec_dim: Dimension W of word vectors.\n",
    "        - hidden_dim: Dimension H for the hidden state of the RNN.\n",
    "        \"\"\"\n",
    "        super(CaptioningRNN, self).__init__()\n",
    "\n",
    "        \n",
    "\n",
    "        vocab_size = len(word_to_idx)\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize word vectors      \n",
    "        self.W_embed=nn.Embedding(vocab_size, wordvec_dim);\n",
    "        \n",
    "        self.vocab_size=vocab_size\n",
    "        self.wordvec_dim=wordvec_dim\n",
    "  \n",
    "        self.W_proj=nn.Linear(input_dim,hidden_dim);\n",
    "\n",
    "        self.num_layers=10\n",
    "        self.rnn=nn.LSTM(input_size  = wordvec_dim, hidden_size =hidden_dim,\n",
    "        num_layers =self.num_layers,batch_first=True,dropout=0.5 );\n",
    "\n",
    "        # Initialize output to vocab weights\n",
    "        self.W_vocab=nn.Linear(hidden_dim,vocab_size);\n",
    "        \n",
    "        \n",
    "        self.criterion=nn.CrossEntropyLoss();\n",
    "\n",
    "\n",
    "        # Cast parameters to correct dtype\n",
    "        #for k, v in self.params.items():\n",
    "        #    self.params[k] = v.astype(self.dtype)\n",
    "        self._null = word_to_idx['<NULL>']\n",
    "        self._start = word_to_idx.get('<START>', None)\n",
    "        self._end = word_to_idx.get('<END>', None)\n",
    "        \n",
    "        self.dtype = dtype\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "    \n",
    "        \n",
    "        hidden_image=self.W_proj(features);\n",
    "        \n",
    "        captions=captions.long()\n",
    "        word_embedding=self.W_embed(captions);        \n",
    "        \n",
    "        \n",
    "        hidden_image=hidden_image.unsqueeze(0)\n",
    "        hidden_image=hidden_image.repeat(self.num_layers,1,1)\n",
    "        \n",
    "        c0=torch.zeros(hidden_image.shape)\n",
    "        if(self.use_cuda):\n",
    "            c0=c0.cuda()\n",
    "        hiddens, _ = self.rnn(word_embedding,(hidden_image,c0))\n",
    "        \n",
    "        \n",
    "        pred=self.W_vocab(hiddens)\n",
    "        \n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    A CaptioningRNN produces captions from image features using a recurrent\n",
    "    neural network.\n",
    "\n",
    "    The RNN receives input vectors of size D, has a vocab size of V, works on\n",
    "    sequences of length T, has an RNN hidden dimension of H, uses word vectors\n",
    "    of dimension W, and operates on minibatches of size N.\n",
    "\n",
    "    Note that we don't use any regularization for the CaptioningRNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_to_idx, input_Dim=1, wordvec_Dim=128,\n",
    "                 hidden_Dim=128, N=128, O=128, image_input=512,set_size=10  ):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.sentence_embedding=MY_CaptioningRNN(word_to_idx, input_dim=input_Dim, wordvec_dim=wordvec_Dim,\n",
    "                 hidden_dim=hidden_Dim);\n",
    "                 \n",
    "        vocab_size=len(word_to_idx);\n",
    "        self.distance_layer_sentences=Distance_Sentences(vocab_size,N,O);\n",
    "        self.distance_layer_images=Distance_Image(vocab_size,N,O,image_input);\n",
    "        \n",
    "        \n",
    "        self.set_size=set_size\n",
    "        self.projection=nn.Linear((self.set_size+1)*O,2);\n",
    "        return \n",
    "        \n",
    "    def forward(self, captions, features):\n",
    "    \n",
    "        \n",
    "        nsamples,trash=captions.shape\n",
    "        \n",
    "        S=self.sentence_embedding.forward( torch.zeros(nsamples,1), captions);\n",
    "        S=S[:,-1,:];\n",
    "        print(S.shape)\n",
    "        S=S.view(S.shape[0]/self.set_size,self.set_size,-1)\n",
    "        print(S.shape)\n",
    "        o_sentence=self.distance_layer_sentences.forward(S);\n",
    "        print(o_sentence.shape)\n",
    "        o_image=self.distance_layer_images.forward(S,features);\n",
    "        print(o_image.shape)\n",
    "        o=torch.cat((o_image, o_sentence), 1);\n",
    "        print(o.shape)\n",
    "        D=nn.functional.log_softmax(self.projection(o),dim=1);\n",
    "        print(D.shape)\n",
    "        return D\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
