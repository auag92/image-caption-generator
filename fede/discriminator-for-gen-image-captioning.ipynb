{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distance_senteces import Distance_Sentences\n",
    "from distance_image import Distance_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptioningModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A CaptioningRNN produces captions from image features using a recurrent\n",
    "    neural network.\n",
    "\n",
    "    The RNN receives input vectors of size D, has a vocab size of V, works on\n",
    "    sequences of length T, has an RNN hidden dimension of H, uses word vectors\n",
    "    of dimension W, and operates on minibatches of size N.\n",
    "\n",
    "    Note that we don't use any regularization for the CaptioningRNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_to_idx, \n",
    "                 input_dim=512, \n",
    "                 wordvec_dim=128,\n",
    "                 hidden_dim=128,\n",
    "                 num_layers=1,\n",
    "                 use_cuda = False, \n",
    "                 device = torch.device(\"cuda:0\")):\n",
    "        \"\"\"\n",
    "        Construct a new CaptioningLSTM instance.\n",
    "\n",
    "        Inputs:\n",
    "        - word_to_idx: A dictionary giving the vocabulary. It contains V entries,\n",
    "          and maps each string to a unique integer in the range [0, V).\n",
    "        - input_dim: Dimension D of input image feature vectors.\n",
    "        - wordvec_dim: Dimension W of word vectors.        \n",
    "        - hidden_dim: Dimension H for the hidden state of the RNN.\n",
    "        \"\"\"\n",
    "        super(CaptioningModel, self).__init__()\n",
    "\n",
    "        \n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            self.use_cuda = True\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            \n",
    "        self.wordvec_dim = wordvec_dim\n",
    "        self.num_layers = num_layers\n",
    "        vocab_size = len(word_to_idx)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.W_embed = nn.Embedding(vocab_size, wordvec_dim) # Initialize word vectors \n",
    "        self.W_proj = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=wordvec_dim, \n",
    "                           hidden_size=hidden_dim, \n",
    "                           num_layers=self.num_layers, \n",
    "                           batch_first=True)\n",
    "\n",
    "        \n",
    "        self.W_vocab = nn.Linear(hidden_dim,vocab_size) # Initialize output to vocab weights\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self._null = word_to_idx['<NULL>']\n",
    "        self._start = word_to_idx.get('<START>', None)\n",
    "        self._end = word_to_idx.get('<END>', None)        \n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        \n",
    "        hidden_image = self.W_proj(features)\n",
    "        word_embedding = self.W_embed(captions)        \n",
    "        hidden_image = hidden_image.unsqueeze(0)\n",
    "        hidden_image = hidden_image.repeat(self.num_layers,1,1)\n",
    "        c0 = torch.zeros(hidden_image.shape).to(self.device)\n",
    "        \n",
    "        hiddens, _ = self.rnn(word_embedding, (hidden_image,c0))\n",
    "        pred = self.W_vocab(hiddens)\n",
    "        return pred\n",
    "\n",
    "\n",
    "    def sample(self, features, max_length=30):\n",
    "        \"\"\"\n",
    "        Run a test-time forward pass for the model, sampling captions for input\n",
    "        feature vectors.\n",
    "\n",
    "        At each timestep, we embed the current word, pass it and the previous hidden\n",
    "        state to the RNN to get the next hidden state, use the hidden state to get\n",
    "        scores for all vocab words, and choose the word with the highest score as\n",
    "        the next word. The initial hidden state is computed by applying an affine\n",
    "        transform to the input image features, and the initial word is the <START>\n",
    "        token.\n",
    "\n",
    "        For LSTMs you will also have to keep track of the cell state; in that case\n",
    "        the initial cell state should be zero.\n",
    "\n",
    "        Inputs:\n",
    "        - features: Array of input image features of shape (N, D).\n",
    "        - max_length: Maximum length T of generated captions.\n",
    "\n",
    "        Returns:\n",
    "        - captions: Array of shape (N, max_length) giving sampled captions,\n",
    "          where each element is an integer in the range [0, V). The first element\n",
    "          of captions should be the first sampled word, not the <START> token.\n",
    "        \"\"\"\n",
    "        N = features.shape[0]\n",
    "        V = self.wordvec_dim\n",
    "        captions = (self._null * torch.ones((N, max_length), dtype=torch.long)).to(self.device)\n",
    "        captions[:, 0] = self._start\n",
    "        \n",
    "        hidden_image = self.W_proj(features);      \n",
    "        next_h = hidden_image.unsqueeze(0)\n",
    "        next_h = next_h.repeat(self.num_layers,1,1)        \n",
    "        \n",
    "        c0 = torch.zeros(next_h.shape).to(self.device)\n",
    "            \n",
    "        for t in range(max_length-1): \n",
    "            word = (self.W_embed(captions[:, t]))\n",
    "            word = word.unsqueeze(1)\n",
    "            pred, out = self.rnn(word,(next_h,c0))\n",
    "            next_h, c0 = out\n",
    "            pred = self.W_vocab(next_h[-1,:,:])     \n",
    "            sol = torch.max(pred, dim=1)\n",
    "            _, captions[:,t + 1] = sol\n",
    "        \n",
    "        return captions.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    A CaptioningRNN produces captions from image features using a recurrent\n",
    "    neural network.\n",
    "\n",
    "    The RNN receives input vectors of size D, has a vocab size of V, works on\n",
    "    sequences of length T, has an RNN hidden dimension of H, uses word vectors\n",
    "    of dimension W, and operates on minibatches of size N.\n",
    "\n",
    "    Note that we don't use any regularization for the CaptioningRNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 word_to_idx, \n",
    "                 input_Dim=1, \n",
    "                 wordvec_Dim=128,\n",
    "                 hidden_Dim=128,\n",
    "                 num_layers=1,\n",
    "                 N=128, \n",
    "                 O=128, \n",
    "                 image_input=512, \n",
    "                 set_size=10, \n",
    "                 use_cuda = False,\n",
    "                 device = torch.device(\"cpu\")):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            self.use_cuda = True\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            \n",
    "        self.sentence_embedding= CaptioningModel(word_to_idx, \n",
    "                                                 input_dim=input_Dim, \n",
    "                                                 wordvec_dim=wordvec_Dim,\n",
    "                                                 num_layers = num_layers,\n",
    "                                                 hidden_dim=hidden_Dim, \n",
    "                                                 use_cuda = self.use_cuda, \n",
    "                                                 device = self.device)         \n",
    "        \n",
    "        vocab_size=len(word_to_idx)\n",
    "        self.distance_layer_sentences = Distance_Sentences(vocab_size, N, O)\n",
    "        self.distance_layer_images = Distance_Image(vocab_size, N, O, image_input)\n",
    "        \n",
    "        \n",
    "        self.set_size = set_size\n",
    "        self.projection = nn.Linear((self.set_size+1)*O, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, captions, features):\n",
    "        \n",
    "        nsamples, _ = captions.shape\n",
    "        \n",
    "        ft = torch.zeros(nsamples,1).to(self.device)\n",
    "        print(\"input to embedding\", ft.shape)\n",
    "        S = self.sentence_embedding.forward(ft, captions)\n",
    "        print(\"Sentence Embedding Shape\", S.shape)\n",
    "        S = S[:,-1,:]\n",
    "        print(\"Sentence Embedding Modified Shape\", S.shape)\n",
    "        \n",
    "        S = S.view(S.shape[0] // self.set_size, self.set_size, -1)\n",
    "        print(S.shape)\n",
    "        \n",
    "        o_sentence = self.distance_layer_sentences.forward(S)\n",
    "        print(\"sentence distance\", o_sentence.shape)\n",
    "\n",
    "        o_image = self.distance_layer_images.forward(S,features)\n",
    "        print(\"image distance\", o_image.shape)\n",
    "        \n",
    "        o = torch.cat((o_image, o_sentence), 1).to(self.device)        \n",
    "        print(\"distances concatenated\", o.shape)\n",
    "        \n",
    "        D = nn.functional.log_softmax(self.projection(o),dim=1)\n",
    "        print(\"logmax performed\", D.shape)\n",
    "        \n",
    "        return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MS-COCO data\n",
    "As in the previous notebook, we will use the Microsoft COCO dataset for captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "idx_to_word <class 'list'> 1004\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n"
     ]
    }
   ],
   "source": [
    "# Load COCO data from disk; this returns a dictionary\n",
    "# We'll work with dimensionality-reduced features for this notebook, but feel\n",
    "# free to experiment with the original features by changing the flag below.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "# Print out all the keys and values from the data dictionary\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = load_coco_data(max_train=20000)\n",
    "minibatch = sample_coco_minibatch(sample_data, batch_size=1000, split='train')\n",
    "\n",
    "captions, features, urls = minibatch\n",
    "captions=torch.from_numpy(captions).long().to(device)\n",
    "features=torch.from_numpy(features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 17])\n",
      "torch.Size([1000, 512])\n",
      "1004\n"
     ]
    }
   ],
   "source": [
    "print(captions.shape)\n",
    "print(features.shape)\n",
    "print(len(sample_data['word_to_idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> a silver train going down some train tracks <END>\n"
     ]
    }
   ],
   "source": [
    "sentence = []\n",
    "for ix in captions[0]:\n",
    "    wrd = sample_data['idx_to_word'][ix]\n",
    "    if wrd in \"<NULL>\":\n",
    "        break\n",
    "    else:\n",
    "        sentence.append(wrd)\n",
    "print(\" \".join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model = Discriminator(word_to_idx=sample_data['word_to_idx'], \n",
    "                                    image_input=features.shape[1], \n",
    "                                    use_cuda=True,\n",
    "                                    device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input to embedding torch.Size([1000, 1])\n",
      "Sentence Embedding Shape torch.Size([1000, 17, 1004])\n",
      "Sentence Embedding Modified Shape torch.Size([1000, 1004])\n",
      "torch.Size([100, 10, 1004])\n",
      "sentence distance torch.Size([100, 1280])\n",
      "image distance torch.Size([100, 128])\n",
      "distances concatenated torch.Size([100, 1408])\n",
      "logmax performed torch.Size([100, 2])\n"
     ]
    }
   ],
   "source": [
    "h = discriminator_model(captions, features[0:100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 948 ms, sys: 390 ms, total: 1.34 s\n",
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "h = torch.sum(h)\n",
    "h.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
