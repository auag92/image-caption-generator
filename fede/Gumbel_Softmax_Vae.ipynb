{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to implement VAE-gumple_softmax in pytorch\n",
    "# author: Devinder Kumar (devinder.kumar@uwaterloo.ca)\n",
    "# The code has been modified from pytorch example vae code \n",
    "# and inspired by the origianl tensorflow implementation of gumble-softmax by Eric Jang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--temp', type=float, default=1.0, metavar='S',\n",
    "                    help='tau(temperature) (default: 1.0)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE MNIST Example\n"
     ]
    }
   ],
   "source": [
    "print('VAE MNIST Example')\n",
    "class Args():\n",
    "    batch_size = 128\n",
    "    epochs = 10\n",
    "    temp = 1.0\n",
    "    no_cuda = False\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    hard = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    if args.cuda:\n",
    "        U = U.cuda()\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"\n",
    "    ST-gumple-softmax\n",
    "    input: [*, n_class]\n",
    "    return: flatten --> [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    \n",
    "    if not hard:\n",
    "#         return y\n",
    "        return y.view(-1, latent_dim * categorical_dim)\n",
    "\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "    y_hard = (y_hard - y).detach() + y\n",
    "#     return y_hard\n",
    "    return y_hard.view(-1, latent_dim * categorical_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_gumbel(nn.Module):\n",
    "    def __init__(self, temp):\n",
    "        super(VAE_gumbel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, latent_dim * categorical_dim)\n",
    "\n",
    "        self.fc4 = nn.Linear(latent_dim * categorical_dim, 256)\n",
    "        self.fc5 = nn.Linear(256, 512)\n",
    "        self.fc6 = nn.Linear(512, 784)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        return self.relu(self.fc3(h2))\n",
    "\n",
    "    def decode(self, z):\n",
    "        h4 = self.relu(self.fc4(z))\n",
    "        h5 = self.relu(self.fc5(h4))\n",
    "        return self.sigmoid(self.fc6(h5))\n",
    "\n",
    "    def forward(self, x, temp, hard):\n",
    "        print(\"x\", x.shape)\n",
    "        q = self.encode(x.view(-1, 784))\n",
    "        print(\"after encoding\", q.shape)\n",
    "        q_y = q.view(q.size(0), latent_dim, categorical_dim)\n",
    "        print(\"after reshaping\", q_y.shape)\n",
    "        z = gumbel_softmax(q_y, temp, hard)\n",
    "        print(\"after gumbel softmax\", z.shape)\n",
    "        return self.decode(z), F.softmax(q_y, dim=-1).reshape(*q.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, qy):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average=False) / x.shape[0]\n",
    "\n",
    "    log_ratio = torch.log(qy * categorical_dim + 1e-20)\n",
    "    KLD = torch.sum(qy * log_ratio, dim=-1).mean()\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 30\n",
    "categorical_dim = 10  # one-of-K vector\n",
    "\n",
    "temp_min = 0.5\n",
    "ANNEAL_RATE = 0.00003\n",
    "\n",
    "model = VAE_gumbel(args.temp)\n",
    "if args.cuda:\n",
    "    model.cuda(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "model.train()\n",
    "train_loss = 0\n",
    "temp = args.temp\n",
    "\n",
    "for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    if args.cuda:\n",
    "        data = data.cuda(device)\n",
    "    if batch_idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([128, 1, 28, 28])\n",
      "after encoding torch.Size([128, 300])\n",
      "after reshaping torch.Size([128, 30, 10])\n",
      "after gumbel softmax torch.Size([128, 300])\n"
     ]
    }
   ],
   "source": [
    "recon_batch, qy = model(data, temp, args.hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784])\n"
     ]
    }
   ],
   "source": [
    "print(recon_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [256/60000 (0%)]\tLoss: 107.393188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nv/hp22/ashanker9/data/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "loss = loss_function(recon_batch, data, qy)\n",
    "loss.backward()\n",
    "train_loss += loss.item() * len(data)\n",
    "optimizer.step()\n",
    "# if batch_idx % 100 == 1:\n",
    "#     temp = np.maximum(temp * np.exp(-ANNEAL_RATE * batch_idx), temp_min)\n",
    "\n",
    "\n",
    "print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "           100. * batch_idx / len(train_loader),\n",
    "           loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    temp = args.temp\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data = data.cuda(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, qy = model(data, temp, args.hard)\n",
    "        loss = loss_function(recon_batch, data, qy)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * len(data)\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 1:\n",
    "            temp = np.maximum(temp * np.exp(-ANNEAL_RATE * batch_idx), temp_min)\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader),\n",
    "                       loss.item()))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    temp = args.temp\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        if args.cuda:\n",
    "            data = data.cuda(device)\n",
    "        recon_batch, qy = model(data, temp, args.hard)\n",
    "        test_loss += loss_function(recon_batch, data, qy).item() * len(data)\n",
    "        if i % 100 == 1:\n",
    "            temp = np.maximum(temp * np.exp(-ANNEAL_RATE * i), temp_min)\n",
    "        if i == 0:\n",
    "            n = min(data.size(0), 8)\n",
    "            comparison = torch.cat([data[:n],\n",
    "                                    recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "            save_image(comparison.data.cpu(),\n",
    "                       'data/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "\n",
    "        M = 64 * latent_dim\n",
    "        np_y = np.zeros((M, categorical_dim), dtype=np.float32)\n",
    "        np_y[range(M), np.random.choice(categorical_dim, M)] = 1\n",
    "        np_y = np.reshape(np_y, [M // latent_dim, latent_dim, categorical_dim])\n",
    "        sample = torch.from_numpy(np_y).view(M // latent_dim, latent_dim * categorical_dim)\n",
    "        if args.cuda:\n",
    "            sample = sample.cuda()\n",
    "        sample = model.decode(sample).cpu()\n",
    "        save_image(sample.data.view(M // latent_dim, 1, 28, 28),\n",
    "                   'data/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nv/hp22/ashanker9/data/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 537.652039\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 256.205170\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 221.186722\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 213.886459\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 205.704697\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 210.139740\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 205.863556\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 205.054214\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 205.187408\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 199.648132\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 208.172195\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 204.667023\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 198.270447\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 195.610199\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 200.066452\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 199.849731\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 200.714066\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 202.974152\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 200.301788\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 198.250473\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 196.933868\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 194.599594\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 193.294632\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 195.743332\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 195.561157\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 201.443863\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 187.216217\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 189.392914\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 187.383652\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 198.820099\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 194.917725\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 190.789383\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 191.655243\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 179.727814\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 181.384644\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 189.188416\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 186.376053\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 191.045303\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 186.665176\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 190.982834\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 190.269028\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 186.612167\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 189.908936\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 190.408035\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 193.192337\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 180.009262\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 189.701920\n",
      "====> Epoch: 1 Average loss: 201.7107\n",
      "====> Test set loss: 185.4369\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 177.951691\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 189.209290\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 183.030853\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 181.178925\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 189.659821\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 174.655838\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 177.665405\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 179.798187\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 182.895859\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 176.068588\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 170.870514\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 166.717468\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 167.567917\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 171.321091\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 171.729706\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 172.686386\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 166.188431\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 161.337677\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 168.156738\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 159.796997\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 163.841309\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 161.569763\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 166.925812\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 164.559570\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 168.592453\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 163.326340\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 159.071930\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 155.636566\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 157.393585\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 160.812195\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 158.174545\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 155.826447\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 153.646713\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 154.129471\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 150.439240\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 158.223816\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 143.810715\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 153.530365\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 155.131699\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 151.536880\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 154.836075\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 160.220795\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 152.420181\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 154.293884\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 145.785187\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 153.708252\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 149.797363\n",
      "====> Epoch: 2 Average loss: 164.3424\n",
      "====> Test set loss: 147.5529\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 146.775604\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 145.640900\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 144.472748\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 142.175491\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 145.802490\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 147.161270\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 147.954849\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 147.445038\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 140.010712\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 139.873184\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 143.647858\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 145.039352\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 142.118546\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 141.138977\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 142.223572\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 139.123703\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 139.433868\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 146.375870\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 138.429184\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 131.356903\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 145.404449\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 141.304199\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 143.574341\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 141.874451\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 143.531540\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 141.576920\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 138.330048\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 139.022400\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 145.511230\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 142.277328\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 134.321915\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 139.697021\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 137.764267\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 138.695602\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 135.013062\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 133.712128\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 139.148499\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 134.318359\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 138.888092\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 136.808441\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 133.476166\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 138.624817\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 140.820877\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 138.139862\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 139.551331\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 140.809296\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 132.942245\n",
      "====> Epoch: 3 Average loss: 141.3725\n",
      "====> Test set loss: 135.9807\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 134.698441\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 134.027969\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 128.412521\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 142.949631\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 134.609711\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 136.736374\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 138.455231\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 137.150391\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 142.813293\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 137.269913\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 137.293320\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 135.580261\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 135.173050\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 129.930435\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 135.142334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 136.205048\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 129.863556\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 128.170197\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 135.692108\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 127.822273\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 136.651077\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 136.246841\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 130.591034\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 128.744598\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 134.469986\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 131.433334\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 130.122360\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 125.520088\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 138.029160\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 134.562454\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 120.790787\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 127.493309\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 133.040817\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 134.980988\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 129.361130\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 130.031189\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 135.321152\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 138.315338\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 137.200470\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 125.645454\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 128.938858\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 125.967789\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 132.930222\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 134.769714\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 130.459030\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 125.777817\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 127.478203\n",
      "====> Epoch: 4 Average loss: 132.8507\n",
      "====> Test set loss: 128.7138\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 128.870377\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 126.675858\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 124.285591\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 133.378281\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 129.037674\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 129.617569\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 131.640671\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 124.557159\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 128.161667\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 124.822266\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 127.921440\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 122.037674\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 125.848465\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 124.115082\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 127.700356\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 120.797279\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 128.252197\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 129.343918\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 127.008911\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 131.206039\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 129.330933\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 120.091568\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 131.505554\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 127.861481\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 123.426476\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 132.032242\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 124.337265\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 131.287598\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 125.669441\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 129.175644\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 125.771225\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 129.807831\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 122.386833\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 119.204788\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 124.348137\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 125.196831\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 126.320518\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 123.978172\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 118.356323\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 125.329773\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 120.732307\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 126.920883\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 126.903122\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 122.430710\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 118.298409\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 121.032677\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 128.288010\n",
      "====> Epoch: 5 Average loss: 126.5661\n",
      "====> Test set loss: 122.8913\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 118.128311\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 123.197060\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 118.796875\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 124.133842\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 124.753258\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 122.254074\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 124.541779\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 124.954453\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 122.058418\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 127.295837\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 125.998795\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 124.059235\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 118.402588\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 121.276154\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 123.021492\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 124.216667\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 120.521835\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 119.972610\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 121.893944\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 121.690674\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 121.918983\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 122.900604\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 115.867615\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 124.816887\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 122.227379\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 125.181145\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 120.789551\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 119.453979\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 123.561249\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 119.478043\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 118.812141\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 126.355667\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 123.936554\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 120.395866\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 122.976685\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 121.894585\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 118.687180\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 125.630074\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 124.513962\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 120.967186\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 122.743309\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 121.609268\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 122.011566\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 117.459366\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 114.832268\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 122.596985\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 116.961113\n",
      "====> Epoch: 6 Average loss: 121.8577\n",
      "====> Test set loss: 118.9422\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 115.224884\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 117.759003\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 121.469833\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 120.111221\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 124.382362\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 119.399887\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 124.734833\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 117.676529\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 120.607857\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 120.120117\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 117.107986\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 118.848816\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 121.020721\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 116.153564\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 115.508377\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 121.721191\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 120.741562\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 118.941765\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 121.732796\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 116.554695\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 122.532715\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 120.503349\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 116.991142\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 116.549606\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 117.218529\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 119.961151\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 118.794601\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 119.242783\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 122.257805\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 119.119392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 119.634399\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 121.222160\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 121.616173\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 120.739792\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 123.769028\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 117.854111\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 115.417442\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 117.746674\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 113.261147\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 114.090500\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 117.125168\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 117.867554\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 118.667244\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 114.186890\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 121.170830\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 114.187851\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 121.132751\n",
      "====> Epoch: 7 Average loss: 118.7791\n",
      "====> Test set loss: 116.5663\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 116.237717\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 116.396782\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 117.395065\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 116.927299\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 117.604340\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 121.284058\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 115.096588\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 117.081482\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 120.538528\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 111.124687\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 121.933846\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 121.253418\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 118.332939\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 116.884216\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 115.727013\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 116.186485\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 114.404892\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 114.175346\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 118.681198\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 115.224876\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 118.524178\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 115.686340\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 114.906860\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 112.536308\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 119.988365\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 117.867905\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 115.929550\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 118.856934\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 119.709885\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 111.499359\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 115.157211\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 115.729362\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 116.385208\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 118.679810\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 119.976837\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 110.636917\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 117.545303\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 114.001633\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 117.029869\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 118.910454\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 114.106293\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 114.061455\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 117.152527\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 114.419479\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 108.755859\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 111.776337\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 115.208466\n",
      "====> Epoch: 8 Average loss: 116.4159\n",
      "====> Test set loss: 114.8641\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 114.460632\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 113.320351\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 115.951363\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 115.132317\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 117.180588\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 115.312363\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 114.360680\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 113.450790\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 113.993568\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 117.744064\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 112.606819\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 116.398750\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 107.656052\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 112.685478\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 118.349823\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 114.614471\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 119.327782\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 119.262611\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 114.482346\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 115.516220\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 115.617432\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 115.477493\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 115.142784\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 115.063004\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 112.330116\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 116.684715\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 119.464478\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 115.581612\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 114.440880\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 113.232132\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 114.620407\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 114.429535\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 112.622566\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 112.000473\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 115.368576\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 109.752365\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 118.580704\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 116.943352\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 117.757141\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 109.371849\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 108.655190\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 113.389519\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 116.656998\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 109.772987\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 111.455116\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 119.545174\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 113.514656\n",
      "====> Epoch: 9 Average loss: 114.3903\n",
      "====> Test set loss: 112.8296\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 115.458206\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 113.370117\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 118.991066\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 111.262016\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 113.539009\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 113.587296\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 113.659271\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 109.763130\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 112.040115\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 116.725929\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 111.715843\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 110.297913\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 110.414497\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 110.907944\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 116.522781\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 109.164551\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 111.438499\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 113.340050\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 112.124443\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 110.071205\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 116.618347\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 113.400391\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 115.776367\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 111.464638\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 114.228531\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 108.797447\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 111.267151\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 110.284538\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 115.039482\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 113.967445\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 113.618378\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 113.026627\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 118.903008\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 114.350464\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 115.179077\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 109.044189\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 109.410889\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 107.844856\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 115.908966\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 111.450714\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 113.856644\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 114.332581\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 116.819679\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 112.845406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 112.396675\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 113.082603\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 109.897362\n",
      "====> Epoch: 10 Average loss: 112.9084\n",
      "====> Test set loss: 112.1237\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
